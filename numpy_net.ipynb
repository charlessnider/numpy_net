{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24324001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy and other basics\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import matplotlib.pyplot as plt\n",
    "import random as random\n",
    "import math as math\n",
    "%matplotlib inline\n",
    "\n",
    "'''\n",
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.activations import linear, relu, sigmoid, softmax\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy, CategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "''';"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc9b645",
   "metadata": {},
   "source": [
    "# theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f99487",
   "metadata": {},
   "source": [
    "## derivatives of output layer versus $m$th layer weights $W^{(m)}_{\\mu\\nu}$\n",
    "\n",
    "Here, $i$ indicates the element of the training set and $j$ indicates which neuron in the output layer we are examining\n",
    "\n",
    "### $\\frac{\\partial a_{ij}^{(n)}}{\\partial W_{\\mu \\nu}^{(n-k)}} = h'(z_{ij}^{(n)})\\left[ \\sum_{\\alpha_1, \\dots, \\alpha_{k-1}} W_{j\\alpha_1}^{(n)}g'(z_{i\\alpha_1}^{(n-1)}) \\dots W_{\\alpha_{\\beta}\\alpha_{\\beta+1}}^{(n-\\beta)}g'(z_{i\\alpha_{\\beta+1}}^{(n-\\beta-1)})\\dots W_{\\alpha_{k-1}\\mu}^{(n-k+1)}g'(z_{i\\mu}^{(n-k)})\\right]a_{i\\nu}^{(n-k-1)}$\n",
    "\n",
    "### define matrices $\\alpha^{(m)}$ such that $\\left( \\alpha^{(m)} \\right)_{ijk} \\equiv W^{(m)}_{jk} g'(z_{ik}^{(m-1)})$\n",
    "\n",
    "Can calculate $\\alpha^{(m)}$ by tiling the array $z$.  $z$ is an $(\\gamma_1, \\gamma_2, 1)$ array for $\\gamma_1$ the number of examples in the input set and $\\gamma_2$ the number of neurons in layer $(m-1)$.  Tile it out with $z_{tile} = $ np.tile(z, (1, 1, $n$)) for $n$ the number of rows of $W$ and then do a broadcast multiplication $\\alpha$ = W * np.transpose(g'($z_{tile}$), (0, 2, 1))\n",
    "\n",
    "### define matrix $\\Lambda_{ijk}^{(n-k)}$ such that $\\Lambda_{ijk}^{(n-k)} = \\left( \\prod_{\\ell=0}^{k-1} \\alpha^{(n-\\ell)}(i,:,:) \\right)_{jk}$\n",
    "\n",
    "with MATLAB notation for the elements of the $\\alpha$s, i.e. $\\alpha(i,:,:)$ is the $i$th slice of $\\alpha$ corresponding to the $i$th element of the ensemble\n",
    "\n",
    "### note that for $k = 0$, $\\Lambda^{(n)}(i,:,:) = I$ (square, size of output) and for $k = 1$, $\\Lambda^{(n-1)}(i,:,:) = \\alpha^{(n)}(i,:,:)$\n",
    "\n",
    "### gives us new shorthand definition for gradients\n",
    "\n",
    "### $\\frac{\\partial a_{ij}^{(n)}}{\\partial W_{\\mu \\nu}^{(n-k)}} = h'(z_{ij}^{(n)}) \\Lambda^{(n-k)}_{ij\\mu} a_{i\\nu}^{(n-k-1)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfddddc",
   "metadata": {},
   "source": [
    "## gradients of cost function $J = \\sum_i \\sum_j L(a^{(n)}_{ij})$ for loss function $L$\n",
    "\n",
    "### $\\frac{\\partial J}{\\partial W_{\\mu\\nu}^{(n-k)}} = \\sum_i \\sum_j L'(a_{ij}^{(n)}) \\frac{\\partial a_{ij}^{(n)}}{\\partial W_{\\mu\\nu}^{(n-k)}} = \\sum_i \\left[ \\sum_j L'(a_{ij}^{(n)}) h'(z_{ij}^{(n)}) \\Lambda^{(n-k)}_{ij\\mu} \\right] a_{i\\nu}^{(n-k-1)}$\n",
    "\n",
    "### define a new matrix $\\eta$ such that $(\\eta)_{ij} = L'(a_{ij}^{(n)}) h'(z_{ij}^{(n)})$\n",
    "\n",
    "### $\\frac{\\partial J}{\\partial W_{\\mu\\nu}^{(n-k)}} = \\sum_i \\left[ \\sum_j \\eta_{ij} \\Lambda^{(n-k)}_{ij\\mu} \\right] a_{i\\nu}^{(n-k-1)}$\n",
    "\n",
    "Transpose $\\Lambda_{ij\\mu}^{(n-k)}$ by cycling 1 and 2 axes: $\\Lambda^{(n-k), T}$ = np.transpose($\\Lambda^{(n-k)}$, (0, 2, 1)) giving us\n",
    "\n",
    "### $\\frac{\\partial J}{\\partial W_{\\mu\\nu}^{(n-k)}} = \\sum_i \\left[ \\sum_j \\Lambda^{(n-k), T}_{i\\mu j} \\eta_{ij} \\right] a_{i\\nu}^{(n-k-1)}$\n",
    "\n",
    "Term in brackets with sum over $j$ can be calculated with numpy multiplication of $\\Lambda$ and $\\eta$, let that be $\\Delta$.  Gives us\n",
    "\n",
    "### $\\frac{\\partial J}{\\partial W_{\\mu\\nu}^{(n-k)}} = \\sum_i \\Delta_{i\\mu} a_{i\\nu}^{(n-k-1)}$\n",
    "\n",
    "This term is obviously a matrix multiplication of $\\Delta^T$ and $a$.  Finally arrive at\n",
    "\n",
    "### gives us shorthand for gradient\n",
    "\n",
    "### $\\frac{\\partial J}{\\partial W_{\\mu\\nu}^{(n-k)}} = (\\Delta^T a^{(n-k-1)})_{\\mu \\nu} $\n",
    "\n",
    "## procedure to calculate gradients wrt weights\n",
    "\n",
    "0) calculate necessary quantities, aka $z$ and $a$ values for each layer\n",
    "\n",
    "1) calculate matrices $\\alpha^{(m)}$\n",
    "\n",
    "2) calculate matrices $\\Lambda^{(n-k)}$\n",
    "\n",
    "3) calculate matrix $\\eta$\n",
    "\n",
    "4) calculate matrix $\\Delta$\n",
    "\n",
    "5) calculate gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17eb2f57",
   "metadata": {},
   "source": [
    "## gradients of biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8c1dee",
   "metadata": {},
   "source": [
    "By the same process as before, can arrive at\n",
    "\n",
    "### $\\frac{\\partial a_{ij}^{(n)}}{\\partial B_{\\mu}^{(n-k)}} = h'(z_{ij}^{(n)})\\left[ \\sum_{\\alpha_1, \\dots, \\alpha_{k-1}} W_{j\\alpha_1}^{(n)}g'(z_{i\\alpha_1}^{(n-1)}) \\dots W_{\\alpha_{\\beta}\\alpha_{\\beta+1}}^{(n-\\beta)}g'(z_{i\\alpha_{\\beta+1}}^{(n-\\beta-1)})\\dots W_{\\alpha_{k-1}\\mu}^{(n-k+1)}g'(z_{i\\mu}^{(n-k)})\\right]$\n",
    "\n",
    "and therefore\n",
    "\n",
    "### $\\frac{\\partial a_{ij}^{(n)}}{\\partial B_{\\mu}^{(n-k)}} = h'(z_{ij}^{(n)}) \\Lambda^{(n-k)}_{ij\\mu}$\n",
    "\n",
    "follow this to the end to get\n",
    "\n",
    "### $\\frac{\\partial J}{\\partial B_{\\mu}^{(n-k)}} = \\sum_i \\Delta_{i\\mu}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f3a832",
   "metadata": {},
   "source": [
    "## attempt to learn a basic problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7599d675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from csv\n",
    "train_data_set = genfromtxt('training_data.csv', delimiter=',')\n",
    "test_data_set = genfromtxt('test_data.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92b0315d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training inputs has shape (42000, 784, 1)\n",
      "The true outputs have shape (42000, 10, 1)\n",
      "The testing inputs has shape (28000, 784, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True label for this image is: 5.0\n"
     ]
    }
   ],
   "source": [
    "# get rid of first row (expand dims to fit network requirements)\n",
    "train_data_set = train_data_set[1:np.shape(train_data_set)[0], :]\n",
    "test_data = np.expand_dims(test_data_set[1:np.shape(test_data_set)[0], :], axis = 2)\n",
    "\n",
    "# separate labels and data\n",
    "train_labels = np.expand_dims(train_data_set[:,0], axis = 1)\n",
    "train_data = np.expand_dims(train_data_set[:,1:np.shape(train_data_set)[1]], axis = 2)\n",
    "\n",
    "# reshape training labels to be the right shape\n",
    "true_labels = np.zeros((np.shape(train_labels)[0], 10, 1))\n",
    "for idx in range(np.shape(train_labels)[0]):\n",
    "    true_labels[idx, int(train_labels[idx][0])] = 1\n",
    "\n",
    "# display size of data\n",
    "print(f'The training inputs has shape {np.shape(train_data)}')\n",
    "print(f'The true outputs have shape {np.shape(true_labels)}')\n",
    "print(f'The testing inputs has shape {np.shape(test_data)}')\n",
    "\n",
    "# dispay a random training data point and label\n",
    "idx = random.randint(0, np.shape(train_data_set)[0])\n",
    "x = np.reshape(train_data[idx,:], (28, 28))\n",
    "x_label = train_labels[idx][0]\n",
    "plt.imshow(x, cmap='hot', interpolation='nearest')\n",
    "plt.show()\n",
    "print(f'True label for this image is: {x_label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "80ebb7e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# reload numpy net\n",
    "import importlib\n",
    "import numpy_net_class\n",
    "importlib.reload(numpy_net_class)\n",
    "from numpy_net_class import *\n",
    "\n",
    "# create network\n",
    "np_net = numpy_net([28*28, 100, 25, 10], 'reLU', 'soft_max', 'cross_entropy', 'cross_entropy')\n",
    "\n",
    "# load data\n",
    "np_net.load_data_set(train_data, true_labels, test_data)\n",
    "\n",
    "# check layer activations\n",
    "# np_net.a0 = np_net.train_input[0:100,:]\n",
    "# np_net.y = np_net.train_output[0:100,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "475c847a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 ====== Current Cost: 15.36999\n",
      "Epoch 1 ====== Current Cost: 40.02176\n",
      "Epoch 2 ====== Current Cost: 146.98145\n",
      "Epoch 3 ====== Current Cost: inf\n",
      "we borked it\n"
     ]
    }
   ],
   "source": [
    "# parameters of training\n",
    "n_epochs = 10\n",
    "batch_size = int(np.shape(train_data)[0] / 100)\n",
    "learn_rate = 0.01\n",
    "\n",
    "# train\n",
    "np_net.train_network(n_epochs, batch_size, learn_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1799,
   "id": "4ed1b334",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_net.calculate_gradJ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dac5a5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1800,
   "id": "157626f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab quantities we want\n",
    "np_net.calculate_layer_activations()\n",
    "tA = np_net.A\n",
    "tZ = np_net.Z\n",
    "ty = np_net.train_output\n",
    "\n",
    "# calculate gradients by hand\n",
    "t_dJdW = np.zeros(np.shape(np_net.weights[-1]))\n",
    "a_iν = tA[2]\n",
    "\n",
    "for μ in range(np.shape(np_net.weights[-1])[0]):\n",
    "    for ν in range(np.shape(np_net.weights[-1])[1]):\n",
    "        \n",
    "        for i in range(np.shape(tA[-1])[0]):\n",
    "            for j in range(np.shape(tA[-1])[1]):\n",
    "                \n",
    "                t_dJdW[μ,ν] += ((1/np.shape(tA[-1])[0]) * (tA[-1][i,j] - ty[i,j]) * a_iν[i,ν] * np.where(j == μ, 1.0, 0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1801,
   "id": "104d39ec",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 43.16218776 -53.16180232 -67.27862259  39.75467639 -99.70995181\n",
      "  35.64353337 -42.2595555   74.93993368 -55.98134749 -87.2838464 ]\n",
      "[ 43.16218776 -53.16180232 -67.27862259  39.75467639 -99.70995181\n",
      "  35.64353337 -42.2595555   74.93993368 -55.98134749 -87.2838464 ]\n"
     ]
    }
   ],
   "source": [
    "print(np_net.dJdW[-1][:,0])\n",
    "print(t_dJdW[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19f9187",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# parameters of training\n",
    "n_epochs = 10\n",
    "batch_size = int(np.shape(train_data)[0] / 100)\n",
    "learn_rate = 0.01\n",
    "\n",
    "# train\n",
    "np_net.train_network(n_epochs, batch_size, learn_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd39d17",
   "metadata": {},
   "source": [
    "## testing ground for class version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6b5f1cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 10, 100)\n",
      "(50, 10, 25)\n",
      "(50, 10, 10)\n",
      "\n",
      "(50, 10, 10)\n",
      "\n",
      "(50, 10, 100)\n",
      "(50, 10, 25)\n",
      "(50, 10, 10)\n",
      "\n",
      "(50, 10, 100)\n",
      "(50, 10, 25)\n",
      "(50, 10, 10)\n"
     ]
    }
   ],
   "source": [
    "idx = random.randint(0, nsamps-1)\n",
    "\n",
    "z = test_net.Z[-1]\n",
    "af = test_net.A[-1]\n",
    "Λ = test_net.Λ\n",
    "for x in Λ:\n",
    "    print(np.shape(x))\n",
    "print()\n",
    "# print(np.shape(z))\n",
    "\n",
    "# make the exponential term\n",
    "zT = np.transpose(z, (0,2,1))\n",
    "# print(np.shape(zT))\n",
    "δz = z - zT\n",
    "print(np.shape(δz))\n",
    "print()\n",
    "\n",
    "# check δz, indexing is (i, ℓ, j) relative to write up\n",
    "for ntrials in range(5000):\n",
    "    \n",
    "    idx = random.randint(0, nsamps - 1)\n",
    "    i = random.randint(0, np.shape(z)[1] - 1)\n",
    "    j = random.randint(0, np.shape(z)[1] - 1)\n",
    "\n",
    "    if not np.isclose(δz[idx,i,j], z[idx,i] - z[idx,j]):\n",
    "        print(δz[idx,i,j])\n",
    "        print(z[idx,j] - z[idx,i])\n",
    "        break\n",
    "        \n",
    "# δz indexing (i, ℓ, j), Λ indexing (i, ℓ, μ)\n",
    "Γ = []\n",
    "for i in range(len(Λ)):\n",
    "    tΓ = np.matmul(np.transpose(δz, (0, 2, 1)), Λ[i]) * af\n",
    "    print(np.shape(tΓ))\n",
    "    Γ.append(np.copy(tΓ))\n",
    "print()\n",
    "\n",
    "# check Γ, indexing is (i, j, μ)\n",
    "for ntrails in range(5000):\n",
    "    \n",
    "    layer = random.randint(0, len(Λ)-1)\n",
    "    i = random.randint(0, nsamps - 1)\n",
    "    j = random.randint(0, np.shape(z)[1] - 1)\n",
    "    μ = random.randint(0, np.shape(z)[1] - 1)\n",
    "    \n",
    "    tΛ = 0\n",
    "    for ℓ in range(10):\n",
    "        tΛ += δz[i,ℓ,j] * Λ[layer][i,ℓ,μ] * af[i,j]\n",
    "        \n",
    "    if not np.isclose(tΛ, Γ[layer][i,j,μ]):\n",
    "        print(tΛ)\n",
    "        print(Γ[layer][i,j,μ])\n",
    "        break\n",
    "        \n",
    "# elementwise multiply by a_ij\n",
    "ξ = []\n",
    "for idx in range(len(Γ)):\n",
    "    \n",
    "    ξ.append(Γ[idx] * af)\n",
    "    print(np.shape(ξ[idx]))\n",
    "    \n",
    "# check ξ, indexing (i,j,μ)\n",
    "for ntrials in range(5000):\n",
    "    \n",
    "    layer = random.randint(0, len(ξ) - 1)\n",
    "    i = random.randint(0, nsamps - 1)\n",
    "    j = random.randint(0, np.shape(af)[1] - 1)\n",
    "    μ = random.randint(0, np.shape(Γ[layer])[2] - 1)\n",
    "    \n",
    "    tΛ = 0\n",
    "    for ℓ in range(10):\n",
    "        tΛ += δz[i,ℓ,j] * Λ[layer][i,ℓ,μ]\n",
    "    \n",
    "    if not np.isclose(Γ[layer][i,j,μ], af[i,j] * tΛ):\n",
    "        print(ξ[layer][i,j,μ])\n",
    "        print(af[i,j] * Γ[layer][i,j,μ])\n",
    "        break\n",
    "        \n",
    "Σ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cdb43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ntrials in range(5000):\n",
    "    \n",
    "    idx = random.randint(0, nsamps - 1)\n",
    "    i = random.randint(0, np.shape(z)[1] - 1)\n",
    "    j = random.randint(0, np.shape(z)[1] - 1)\n",
    "\n",
    "    if np.array_equal(z_ij[idx,:,i], z_ij[idx,:,j]):\n",
    "        print(\"bruh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d277fd33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 10, 100)\n",
      "(50, 10, 25)\n",
      "(50, 10, 10)\n",
      "\n",
      "(50, 100, 1)\n",
      "(50, 25, 1)\n",
      "(50, 10, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx = random.randint(0, nsamps-1)\n",
    "\n",
    "af = test_net.A[-1]\n",
    "Λ = test_net.Λ\n",
    "for x in Λ:\n",
    "    print(np.shape(x))\n",
    "print()\n",
    "        \n",
    "# δz indexing (i, ℓ, j), Λ indexing (i, ℓ, μ)\n",
    "Γ = []\n",
    "for i in range(len(Λ)):\n",
    "    tΓ = np.matmul(np.transpose(Λ[i], (0, 2, 1)), af)\n",
    "    print(np.shape(tΓ))\n",
    "    Γ.append(np.copy(tΓ))\n",
    "print()\n",
    "\n",
    "# check Γ, indexing is (i, j, μ)\n",
    "for ntrails in range(5000):\n",
    "    \n",
    "    layer = random.randint(0, len(Λ)-1)\n",
    "    i = random.randint(0, nsamps - 1)\n",
    "    μ = random.randint(0, np.shape(Λ[layer])[2] - 1)\n",
    "    \n",
    "    tΛ = 0\n",
    "    for ℓ in range(10):\n",
    "        tΛ += Λ[layer][i,ℓ,μ] * af[i,ℓ]\n",
    "        \n",
    "    if not np.isclose(tΛ, Γ[layer][i,μ]):\n",
    "        print(tΛ)\n",
    "        print(Γ[layer][i,μ])\n",
    "        break\n",
    "        \n",
    "# subtract off\n",
    "ξ = []\n",
    "for idx in range(len(Λ)):\n",
    "    ξ.append(Λ[idx] - np.transpose(Γ[idx], (0,2,1)))\n",
    "    \n",
    "# check ξ\n",
    "for ntrails in range(5000):\n",
    "    \n",
    "    layer = random.randint(0, len(Λ) - 1)\n",
    "    i = random.randint(0, nsamps - 1)\n",
    "    j = random.randint(0, np.shape(af)[1] - 1)\n",
    "    μ = random.randint(0, np.shape(Λ[layer])[2] - 1)\n",
    "    \n",
    "    if not np.isclose(ξ[layer][i,j,μ], Λ[layer][i,j,μ] - Γ[layer][i,μ]):\n",
    "        print(ξ[layer][i,j,μ])\n",
    "        print(Λ[layer][i,j,μ] - Γ[layer][i,μ])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d1e9dd5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean value of the error in weights for layer 1 is 0.0001375311%\n",
      "The standard deviation of the error in weights for layer 1 is 0.0031890041%\n",
      "The median value of the error in weights for layer 1 is 0.0000052368%\n",
      "\n",
      "The mean value of the error in weights for layer 2 is 0.0000412847%\n",
      "The standard deviation of the error in weights for layer 2 is 0.0002229049%\n",
      "The median value of the error in weights for layer 2 is 0.0000053616%\n",
      "\n",
      "The mean value of the error in weights for layer 3 is 100.0671489692%\n",
      "The standard deviation of the error in weights for layer 3 is 99.9337397358%\n",
      "The median value of the error in weights for layer 3 is 102.6909909759%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run it with the network\n",
    "import importlib\n",
    "import numpy_net_class\n",
    "importlib.reload(numpy_net_class)\n",
    "from numpy_net_class import *\n",
    "import copy\n",
    "\n",
    "# size of input\n",
    "input_sz = 28*28\n",
    "\n",
    "# activation, loss functions\n",
    "g_func = 'reLU'\n",
    "h_func = 'soft_max'\n",
    "JL_func = 'cross_entropy'\n",
    "\n",
    "'''\n",
    "μ_n0p1 = 0\n",
    "μ_n1p0 = 0\n",
    "\n",
    "σ_n0p1 = 0\n",
    "σ_n1p0 = 0\n",
    "'''\n",
    "\n",
    "for ntrials in range(1):\n",
    "\n",
    "    # create network\n",
    "    test_net = numpy_net([input_sz, 100, 25, 10], g_func, h_func, JL_func, JL_func)\n",
    "\n",
    "    # fetch weights, put into separate list for resetting\n",
    "    W = []\n",
    "    B = []\n",
    "    for idx in range(len(test_net.weights)):\n",
    "        W.append(np.copy(test_net.weights[idx]))\n",
    "        B.append(np.copy(test_net.biases[idx]))\n",
    "\n",
    "    # initialize with subset of data (randomly select 50 training examples)\n",
    "    nsamps = 2;\n",
    "    batch_idx = random.sample(range(0, np.shape(train_data)[0]), nsamps)\n",
    "\n",
    "    # set a0, y\n",
    "    a0 = train_data[batch_idx,:]\n",
    "    y = true_labels[batch_idx,:]\n",
    "\n",
    "    # assign\n",
    "    test_net.a0 = a0\n",
    "    test_net.y = y\n",
    "\n",
    "    # calculate gradients\n",
    "    test_net.calculate_gradJ()\n",
    "\n",
    "    # small shift for gradients\n",
    "    p = 9\n",
    "    dWk_ij = np.random.uniform(1 / (10 ** p), 1 / (10 ** (p - 1)), 1)[0]\n",
    "\n",
    "    # to save info\n",
    "    dJdW = []\n",
    "\n",
    "    # loop over the layers\n",
    "    for k in range(len(W)):\n",
    "\n",
    "        # calculated grads\n",
    "        t_dJdW = np.zeros((np.shape(W[k])[0], np.shape(W[k])[1]))\n",
    "\n",
    "        # loop over elements of weights\n",
    "        for i in range(np.shape(W[k])[0]):\n",
    "            for j in range(np.shape(W[k])[1]):\n",
    "\n",
    "                # reset weights and biases\n",
    "                t_W = copy.deepcopy(W)\n",
    "                t_B = copy.deepcopy(B)\n",
    "\n",
    "                # small adjustment\n",
    "                t_W[k][i,j] -= dWk_ij\n",
    "\n",
    "                # predict\n",
    "                t_a0 = copy.deepcopy(a0)\n",
    "                for idx in range(len(t_W)-1):\n",
    "\n",
    "                    # apply weights\n",
    "                    t_a0 = g(np.matmul(t_W[idx], t_a0) + t_B[idx], g_func)\n",
    "\n",
    "                # final output\n",
    "                t_a0 = np.matmul(t_W[-1], t_a0) + t_B[-1]\n",
    "                t_af = g(t_a0, h_func)\n",
    "\n",
    "                # calculate cost\n",
    "                J0 = J(t_af, y, JL_func)\n",
    "\n",
    "                # small adjustment\n",
    "                t_W = copy.deepcopy(W)\n",
    "                t_W[k][i,j] += dWk_ij\n",
    "\n",
    "                # predict\n",
    "                t_a0 = copy.deepcopy(a0)\n",
    "                for idx in range(len(t_W)-1):\n",
    "\n",
    "                    # apply weights\n",
    "                    t_a0 = g(np.matmul(t_W[idx], t_a0) + t_B[idx], g_func)\n",
    "\n",
    "                # final output\n",
    "                t_a0 = np.matmul(t_W[-1], t_a0) + t_B[-1]\n",
    "                t_af = g(t_a0, h_func)\n",
    "\n",
    "                # calculate cost\n",
    "                Jf = J(t_af, y, JL_func)\n",
    "\n",
    "                # calculate approximate gradient\n",
    "                t_dJdW[i,j] = (Jf-J0)/(2 * dWk_ij)\n",
    "\n",
    "        dJdW.append(np.copy(t_dJdW))\n",
    "\n",
    "    δ = []\n",
    "    for k in range(len(W)):\n",
    "        μval = (np.abs(test_net.dJdW[k]) + np.abs(dJdW[k])) / 2\n",
    "        μval = np.where(μval > 0.0, μval, 1.0)\n",
    "        δ.append(np.abs(test_net.dJdW[k] - dJdW[k]) / μval)\n",
    "\n",
    "    for k in range(len(W)):\n",
    "\n",
    "        # want to only look at places where they are not identically zero from the reLU\n",
    "        t_bool = (dJdW[k] != 0) | (test_net.dJdW[k] != 0)\n",
    "\n",
    "        print(f'The mean value of the error in weights for layer {k+1} is {100 * np.average(δ[k][t_bool]):.10f}%')\n",
    "        print(f'The standard deviation of the error in weights for layer {k+1} is {100 * np.std(δ[k][t_bool]):.10f}%')\n",
    "        print(f'The median value of the error in weights for layer {k+1} is {100 * np.median(δ[k][t_bool]):.10f}%')\n",
    "        print()\n",
    "       \n",
    "    '''\n",
    "    t_bool1 = (dJdW[0] != 0) | (test_net.dJdW[0] != 0)\n",
    "    t_bool2 = (dJdW[1] != 0) | (test_net.dJdW[1] != 0)\n",
    "        \n",
    "    if (100 * np.average(δ[0][t_bool1]) > 0.1) | (100 * np.average(δ[1][t_bool2]) > 0.1):\n",
    "        μ_n0p1 += 1\n",
    "        if (100 * np.average(δ[0][t_bool1]) > 1) | (100 * np.average(δ[1][t_bool2]) > 1):\n",
    "            μ_n1p0 += 1\n",
    "            \n",
    "    if (100 * np.std(δ[0][t_bool1]) > 0.1) | (100 * np.std(δ[1][t_bool2]) > 0.1):\n",
    "        σ_n0p1 += 1\n",
    "        if (100 * np.std(δ[0][t_bool1]) > 1) | (100 * np.std(δ[1][t_bool2]) > 1):\n",
    "            σ_n1p0 += 1        \n",
    "    # print(\"|=======================================================================================================|\")\n",
    "    \n",
    "print(f'Number of instances with average error in layers 1 and 2 greater than 0.1%: {μ_n0p1}')\n",
    "print(f'Number of instances with average error in layers 1 and 2 greater than 1.0%: {μ_n1p0}')\n",
    "print()\n",
    "print(f'Number of instances with standard deviation in error in layers 1 and 2 greater than 0.1%: {σ_n0p1}')\n",
    "print(f'Number of instances with standard deviation in error in layers 1 and 2 greater than 1.0%: {σ_n1p0}') \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9361bf27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean value of the error in weights for layer 1 is 0.0167238755%\n",
      "The standard deviation of the error in weights for layer 1 is 0.0288447438%\n",
      "The median value of the error in weights for layer 1 is 0.0062831091%\n",
      "\n",
      "The mean value of the error in weights for layer 2 is 0.0078626281%\n",
      "The standard deviation of the error in weights for layer 2 is 0.0121532246%\n",
      "The median value of the error in weights for layer 2 is 0.0017138665%\n",
      "\n",
      "The mean value of the error in weights for layer 3 is 0.0004516266%\n",
      "The standard deviation of the error in weights for layer 3 is 0.0004121857%\n",
      "The median value of the error in weights for layer 3 is 0.0003131859%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run it with the network\n",
    "import importlib\n",
    "import numpy_net_class\n",
    "importlib.reload(numpy_net_class)\n",
    "from numpy_net_class import *\n",
    "import copy\n",
    "\n",
    "# size of input\n",
    "input_sz = 28*28\n",
    "\n",
    "# activation, loss functions\n",
    "g_func = 'reLU'\n",
    "h_func = 'soft_max'\n",
    "JL_func = 'cross_entropy'\n",
    "\n",
    "# create network\n",
    "test_net = numpy_net([input_sz, 100, 25, 10], g_func, h_func, JL_func, JL_func)\n",
    "\n",
    "# fetch weights, put into separate list for resetting\n",
    "W = []\n",
    "B = []\n",
    "for idx in range(len(test_net.weights)):\n",
    "    W.append(np.copy(test_net.weights[idx]))\n",
    "    B.append(np.copy(test_net.biases[idx]))\n",
    "\n",
    "# initialize with subset of data (randomly select 50 training examples)\n",
    "nsamps = 50;\n",
    "batch_idx = random.sample(range(0, np.shape(train_data)[0]), nsamps)\n",
    "\n",
    "# set a0, y\n",
    "a0 = train_data[batch_idx,:]\n",
    "y = true_labels[batch_idx,:]\n",
    "\n",
    "# assign\n",
    "test_net.a0 = a0\n",
    "test_net.y = y\n",
    "\n",
    "# calculate gradients\n",
    "test_net.calculate_gradJ()\n",
    "\n",
    "# small shift for gradients\n",
    "p = 9\n",
    "dBk_i = np.random.uniform(1 / (10 ** p), 1 / (10 ** (p - 1)), 1)[0]\n",
    "\n",
    "# to save info\n",
    "dJdB = []\n",
    "\n",
    "# loop over the layers\n",
    "for k in range(len(B)):\n",
    "\n",
    "    # calculated grads\n",
    "    t_dJdB = np.zeros((np.shape(B[k])[0], 1))\n",
    "\n",
    "    # loop over elements of weights\n",
    "    for i in range(np.shape(B[k])[0]):\n",
    "\n",
    "        # reset weights and biases\n",
    "        t_W = copy.deepcopy(W)\n",
    "        t_B = copy.deepcopy(B)\n",
    "\n",
    "        # small adjustment\n",
    "        t_B[k][i] -= dBk_i\n",
    "\n",
    "        # predict\n",
    "        t_a0 = copy.deepcopy(a0)\n",
    "        for idx in range(len(t_W)-1):\n",
    "\n",
    "            # apply weights\n",
    "            t_a0 = g(np.matmul(t_W[idx], t_a0) + t_B[idx], g_func)\n",
    "\n",
    "        # final output\n",
    "        t_a0 = np.matmul(t_W[-1], t_a0) + t_B[-1]\n",
    "        t_af = g(t_a0, h_func)\n",
    "\n",
    "        # calculate cost\n",
    "        J0 = J(t_af, y, JL_func)\n",
    "\n",
    "        # small adjustment\n",
    "        t_B = copy.deepcopy(B)\n",
    "        t_B[k][i] += dBk_i\n",
    "\n",
    "        # predict\n",
    "        t_a0 = copy.deepcopy(a0)\n",
    "        for idx in range(len(t_W)-1):\n",
    "\n",
    "            # apply weights\n",
    "            t_a0 = g(np.matmul(t_W[idx], t_a0) + t_B[idx], g_func)\n",
    "\n",
    "        # final output\n",
    "        t_a0 = np.matmul(t_W[-1], t_a0) + t_B[-1]\n",
    "        t_af = g(t_a0, h_func)\n",
    "\n",
    "        # calculate cost\n",
    "        Jf = J(t_af, y, JL_func)\n",
    "\n",
    "        # calculate approximate gradient\n",
    "        t_dJdB[i] = (Jf-J0)/(2 * dBk_i)\n",
    "            \n",
    "    dJdB.append(np.copy(t_dJdB))\n",
    "\n",
    "δ = []\n",
    "for k in range(len(W)):\n",
    "    μval = (np.abs(test_net.dJdB[k]) + np.abs(dJdB[k])) / 2\n",
    "    μval = np.where(μval > 0.0, μval, 1.0)\n",
    "    δ.append(np.abs(test_net.dJdB[k] - dJdB[k]) / μval)\n",
    "\n",
    "for k in range(len(W)):\n",
    "    \n",
    "    # want to only look at places where they are not identically zero from the reLU\n",
    "    t_bool = (dJdB[k] != 0) | (test_net.dJdB[k] != 0)\n",
    "    \n",
    "    print(f'The mean value of the error in weights for layer {k+1} is {100 * np.average(δ[k][t_bool]):.10f}%')\n",
    "    print(f'The standard deviation of the error in weights for layer {k+1} is {100 * np.std(δ[k][t_bool]):.10f}%')\n",
    "    print(f'The median value of the error in weights for layer {k+1} is {100 * np.median(δ[k][t_bool]):.10f}%')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e891315f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def J(a, y, func):\n",
    "\n",
    "    # mean square error\n",
    "    if func == 'mean_square_error':\n",
    "        return (1/(2 * np.shape(a)[0])) * np.sum(np.square(a - y))\n",
    "\n",
    "    # soft max\n",
    "    elif func == 'cross_entropy':\n",
    "\n",
    "        # adjust a to filter really small values\n",
    "        # t_a = np.where(a > 10**-20, a, 10**-20)\n",
    "\n",
    "        # filter to make sure rounding doesn't mess with us\n",
    "        t_L = np.where(y > 0.5, -y * np.log(a), 0.0)\n",
    "\n",
    "        # do the sum\n",
    "        return np.sum(t_L) / np.shape(a)[0]\n",
    "\n",
    "    # default to mse\n",
    "    else:\n",
    "        return (1/(2 * np.shape(a)[0])) * np.sum(np.square(a - y))\n",
    "    \n",
    "def g(z, func):\n",
    "\n",
    "    # linear\n",
    "    if func == 'linear':\n",
    "        return z\n",
    "\n",
    "    # sigmod\n",
    "    elif func == 'sigmoid':\n",
    "        return 1/(1 + np.exp(-z))\n",
    "\n",
    "    # reLU\n",
    "    elif func == 'reLU':\n",
    "        return np.where(z > 0.0, z, 0.0)\n",
    "\n",
    "    # softmax\n",
    "    elif func == 'soft_max':\n",
    "\n",
    "        # calculate max z value to shift everything\n",
    "        z_max = np.reshape(np.amax(z, axis = 1), (np.shape(z)[0], 1, 1))\n",
    "\n",
    "        # calculate exponents\n",
    "        exp_z = np.exp(z - z_max)\n",
    "\n",
    "        # calculate slice sums (have to do some reshaping for broadcasting\n",
    "        sum_exp_z = np.reshape(np.sum(exp_z, axis = 1), (np.shape(exp_z)[0], 1, 1))\n",
    "\n",
    "        # combine\n",
    "        return exp_z / sum_exp_z\n",
    "\n",
    "    # default to reLU for all other inputs\n",
    "    else:\n",
    "        return np.where(z > 0.0, z, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1879,
   "id": "f95d363f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 784, 1)\n",
      "(500, 1, 1)\n",
      "(500, 784, 1)\n",
      "(500, 1, 1)\n",
      "(500, 784, 1)\n"
     ]
    }
   ],
   "source": [
    "# run it with the network\n",
    "import importlib\n",
    "import numpy_net_class\n",
    "importlib.reload(numpy_net_class)\n",
    "from numpy_net_class import *\n",
    "import copy\n",
    "\n",
    "# size of input\n",
    "input_sz = 28*28\n",
    "\n",
    "# activation, loss functions\n",
    "g_func = 'reLU'\n",
    "h_func = 'soft_max'\n",
    "JL_func = 'cross_entropy'\n",
    "\n",
    "# create network\n",
    "test_net = numpy_net([input_sz, 100, 25, 10], g_func, h_func, JL_func, JL_func)\n",
    "\n",
    "# fetch weights, put into separate list for resetting\n",
    "W = []\n",
    "B = []\n",
    "for idx in range(len(test_net.weights)):\n",
    "    W.append(np.copy(test_net.weights[idx]))\n",
    "    B.append(np.copy(test_net.biases[idx]))\n",
    "\n",
    "# initialize with subset of data (randomly select 50 training examples)\n",
    "nsamps = 50;\n",
    "batch_idx = random.sample(range(0, np.shape(train_data)[0]), nsamps)\n",
    "\n",
    "# set a0, y\n",
    "a0 = train_data[batch_idx,:]\n",
    "y = true_labels[batch_idx,:]\n",
    "\n",
    "# assign\n",
    "test_net.a0 = a0\n",
    "test_net.y = y\n",
    "\n",
    "# calculate gradients\n",
    "test_net.calculate_gradJ()\n",
    "\n",
    "##############################################################################################################################\n",
    "##############################################################################################################################\n",
    "\n",
    "# small shift for gradients\n",
    "p = 9\n",
    "dWk_ij = np.random.uniform(1 / (10 ** p), 1 / (10 ** (p - 1)), 1)[0]\n",
    "\n",
    "# to save info\n",
    "dJdW = []\n",
    "\n",
    "# loop over the layers\n",
    "for k in range(len(W)):\n",
    "\n",
    "    # calculated grads\n",
    "    t_dJdW = np.zeros((np.shape(W[k])[0], np.shape(W[k])[1]))\n",
    "\n",
    "    # loop over elements of weights\n",
    "    for i in range(np.shape(W[k])[0]):\n",
    "        for j in range(np.shape(W[k])[1]):\n",
    "\n",
    "            # reset weights and biases\n",
    "            t_W = copy.deepcopy(W)\n",
    "            t_B = copy.deepcopy(B)\n",
    "\n",
    "            # small adjustment\n",
    "            t_W[k][i,j] -= dWk_ij\n",
    "\n",
    "            # predict\n",
    "            t_a0 = copy.deepcopy(a0)\n",
    "            for idx in range(len(t_W)-1):\n",
    "\n",
    "                # apply weights\n",
    "                t_a0 = g(np.matmul(t_W[idx], t_a0) + t_B[idx], g_func)\n",
    "\n",
    "            # final output\n",
    "            t_a0 = np.matmul(t_W[-1], t_a0) + t_B[-1]\n",
    "            t_af = g(t_a0, h_func)\n",
    "\n",
    "            # calculate cost\n",
    "            J0 = J(t_af, y, JL_func)\n",
    "\n",
    "            # small adjustment\n",
    "            t_W = copy.deepcopy(W)\n",
    "            t_W[k][i,j] += dWk_ij\n",
    "\n",
    "            # predict\n",
    "            t_a0 = copy.deepcopy(a0)\n",
    "            for idx in range(len(t_W)-1):\n",
    "\n",
    "                # apply weights\n",
    "                t_a0 = g(np.matmul(t_W[idx], t_a0) + t_B[idx], g_func)\n",
    "\n",
    "            # final output\n",
    "            t_a0 = np.matmul(t_W[-1], t_a0) + t_B[-1]\n",
    "            t_af = g(t_a0, h_func)\n",
    "\n",
    "            # calculate cost\n",
    "            Jf = J(t_af, y, JL_func)\n",
    "\n",
    "            # calculate approximate gradient\n",
    "            t_dJdW[i,j] = (Jf-J0)/(2 * dWk_ij)\n",
    "            \n",
    "    dJdW.append(np.copy(t_dJdW))\n",
    "\n",
    "δ = []\n",
    "for k in range(len(W)):\n",
    "    μval = (np.abs(test_net.dJdW[k]) + np.abs(dJdW[k])) / 2\n",
    "    μval = np.where(μval > 0.0, μval, 1.0)\n",
    "    δ.append(np.abs(test_net.dJdW[k] - dJdW[k]) / μval)\n",
    "\n",
    "for k in range(len(W)):\n",
    "    \n",
    "    # want to only look at places where they are not identically zero from the reLU\n",
    "    t_bool = (dJdW[k] != 0) & (test_net.dJdW[k] != 0)\n",
    "    \n",
    "    print(f'The mean value of the error in weights for layer {k+1} is {100 * np.average(δ[k][t_bool]):.10f}%')\n",
    "    print(f'The standard deviation of the error in weights for layer {k+1} is {100 * np.std(δ[k][t_bool]):.10f}%')\n",
    "    print(f'The median value of the error in weights for layer {k+1} is {100 * np.median(δ[k][t_bool]):.10f}%')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf67b068",
   "metadata": {},
   "source": [
    "# with tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22b2904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same but with tensorflow\n",
    "model = Sequential([        \n",
    "    tf.keras.layers.InputLayer(input_shape = (28*28,)),\n",
    "    tf.keras.layers.Dense(units = 50, activation = 'relu', name = \"L1\"),\n",
    "    tf.keras.layers.Dense(units = 25, activation = 'relu', name = \"L2\"),\n",
    "    tf.keras.layers.Dense(units = 10, activation = 'softmax', name = \"L3\")\n",
    "    ], name = \"my_model\")\n",
    "\n",
    "model.build()\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    loss = CategoricalCrossentropy(),\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74328c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from csv\n",
    "train_data_set = genfromtxt('training_data.csv', delimiter=',')\n",
    "test_data_set = genfromtxt('test_data.csv', delimiter=',')\n",
    "\n",
    "# clear first row\n",
    "train_data_set = train_data_set[1:np.shape(train_data_set)[0], :]\n",
    "\n",
    "# separate labels and data\n",
    "train_labels = train_data_set[:,0]\n",
    "train_data = train_data_set[:,1:np.shape(train_data_set)[1]] / 255.0 # normalize\n",
    "\n",
    "# reshape training labels to be the right shape\n",
    "true_labels = np.zeros((np.shape(train_labels)[0], 10,))\n",
    "for idx in range(np.shape(train_labels)[0]):\n",
    "    true_labels[idx, int(train_labels[idx])] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b86ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_data, true_labels, epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
